# -*- coding: utf-8 -*-
"""Trabalho IA - A3 - Predicao Ingredientes

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P3YqYe21OrEx0vpVyWBKSUCIIl_3nRvF

# **NeuraChef**

Essa é uma rede neural de previsão de gasto de ingredientes por dia em um restaurante utilizando as seguintes **variaveis**
* Estação do ano
* Clima
* Dia da semana
* Dia do mês
* Valor do ingrediente
* Consumo nas receitas

<br>

---

## **Classe Ingredientes**

A classe Ingredientes herda da classe Dataset, uma classe do pytorch para criação de datasets.

Temos 3 métodos nessa classe, sendo o método __ init __ o construtor, que recebe um caminho para o csv que será criado o data set e com a classe do pandas lê o arquivo no caminho especificado.

O método __ getitem __ que separa o dataset em sample e label que seria o dado e o rótulo respectivamente, neste caso, os dados estão da coluna 1 até a coluna 9 do csv, enquanto o rótulo, está na ultima coluna do csv.

E por ultimo temos o método __ len __ que retorna o tamanho do conjunto de dados.
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch import nn
from torch import optim

from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from sklearn import metrics
from sklearn.preprocessing import StandardScaler

import pandas as pd
import numpy as np
import time
import os

from sklearn.metrics import r2_score

import pickle

import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import files


class Ingrediente(Dataset):
  def __init__(self, csv_path, scaler_feat=None, scaler_label=None):

    self.dados = pd.read_csv(csv_path).to_numpy()

  def __getitem__(self, idx):

    sample = self.dados[idx][1:9]
    label  = self.dados[idx][-1:]

    sample = torch.from_numpy(sample.astype(np.float32))
    label  = torch.from_numpy(label.astype(np.float32))

    return sample, label

  def __len__(self):
    return len(self.dados)

"""## **Classe MLP ( Multilayer Perceptron )**

A classe MLP herda da classe nn.Module que é uma classe do pytorch que auxilia na criação das redes neruais.

Temos 2 métodos nessa classe, sendo o método __ init __ o construtor que recebe o input_size (tamanho da entrada), o hidden_size (tamanho da camada escondida) e o out_size(tamanho da saida). Neste método é definida a arquitetura que será utilizada na rede neural. Primeiramente, é utilizado o nn.Sequential pra criar uma sequencia de camadas, sendo essas camadas:

### **Extração de caracteristicas**

**Camada de entrada**

*   nn.Linear -> uma camada linear que recebe a entrada e passa para a camada escondida
*   nn.ReLU -> uma função de ativação que faz com que a rede possa prever de forma não linear

**Camada escondida**

*   nn.Linear -> uma camada linear que recebe o tamanho da camada escondida e tem uma saida de mesmo tamanho para a camada de saída
*   nn.ReLU -> Outra função de ativação ReLU com a mesma funcionalidade

### **Classificação da rede**

**Camada de saída**

*   nn.Linear -> uma camada linear que recebe o tamanho da camada escondida e tem uma saida de 1 que é o tamanho da saida

*  nn.ReLU -> Outra função de ativação ReLU

O método forward ele serve para indicar como os dados devem fluir pela rede, neste caso, os dados de entrada passam pela camada escondida e são classificados, gerando assim um resultado na camada de saída.



"""

class MLP(nn.Module):

  def __init__(self, input_size, hidden_size, out_size):
    super(MLP, self).__init__()

    self.features = nn.Sequential(
          nn.Linear(input_size, hidden_size),
          nn.ReLU(),
          nn.Linear(hidden_size, hidden_size),
          nn.ReLU(),
    )

    self.classifier = nn.Sequential(
        nn.Linear(hidden_size, out_size),
        nn.ReLU(),
    )

  def forward(self, X):

    hidden = self.features(X)
    output = self.classifier(hidden)

    return output

"""# **Funções de treinamento, validação e medição de precisão**


### Função train()

A função train recebe um train_loader, com os dados de treino, uma rede e a época.
Essa função tem como objetivo treinar a rede.
Primeiramente a função coloca a rede em modo de treinamento através do net.train().
É utilizado um loop dentro da funcao que itera sobre o train_loader, dentro deste loop é feito o **forward**, que é basicamente usar a rede para prever um dado a partir de uma entrada, após essa predição é utilizado uma função de perda, que utiliza a predição + o rotulo para calcular o loss, e por fim essa loss é adicionada a uma lista com a loss da epoch.
Após o forward, temos o **backpropagation**, que utiliza de calculos de derivada para calcular os parametros mais adequados ao modelo que estamos tentando prever.
Por fim temos a atualizacao dos parametros calculados anteriormente utilizando o otimizador.

<br>

### Função validate()

A função validate recebe um test_loader com dados de teste do modelo, uma rede e uma época.
Essa função tem como objetivo validar o desempenho da rede e do treinamento do modelo.
Primeiramente a função coloca a rede em modo de validação a partir do net.eval().
É utilizado um loop dentro da função que itera sobre o test_loader e assim como a função train() é realizado o **forward** que utiliza a rede para prever o dado e calcula o loss a partir de uma função. Na funcao validation, diferente da função train, não temos o backpropagation.

<br>

### Função calculate_precision()

A função calculate_precision recebe um test_loader com dados de teste do modelo, uma rede.
O objetivo desta função é calcular a precisão do modelo através do calculo do coeficiente de determinação (R²), esse coeficiente varia de 0 a 1 podendo ter resultados negativos caso a rede esteja com resultados muito distantes do que foi testado.
Assim como na função validate, a rede é colocada em modo de validação. Após isso, é feito um iterando em test_loader e armazenando os dados de predição e teste.
Após isso, é utilizado a função r2_score do scikit-learn para calcular o coeficiente R2.

"""

def train(train_loader, net, epoch):

  net.train()

  start = time.time()

  epoch_loss  = []
  for batch in train_loader:

    dado, rotulo = batch

    dado = dado.to(args['device'])
    rotulo = rotulo.to(args['device'])

    # Forward
    ypred = net(dado)
    loss = criterion(ypred, rotulo)
    epoch_loss.append(loss.cpu().data)

    # Backpropagation
    loss.backward()
    optimizer.step()

  epoch_loss = np.asarray(epoch_loss)

  end = time.time()
  print('EPOCH: %d'% (epoch))
  print('TRAINING -> Loss: %.2f +/- %.2f, Time: %.2f ' % (epoch_loss.mean(), epoch_loss.std(), end-start))


  return epoch_loss.mean()

def validate(test_loader, net, epoch):

  net.eval()

  start = time.time()

  epoch_loss  = []

  with torch.no_grad():
    for batch in test_loader:

      dado, rotulo = batch

      dado = dado.to(args['device'])
      rotulo = rotulo.to(args['device'])

      # Forward
      ypred = net(dado)
      loss = criterion(ypred, rotulo)
      epoch_loss.append(loss.cpu().data)

  epoch_loss = np.asarray(epoch_loss)

  end = time.time()
  print('VALIDATION -> Loss: %.2f +/- %.2f, Time: %.2f ' % (epoch_loss.mean(), epoch_loss.std(), end - start))

  return epoch_loss.mean()

def calculate_precision(test_loader, net):
    net.eval()

    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in test_loader:
            dado, rotulo = batch
            dado = dado.to(args['device'])

            ypred = net(dado)

            y_true.extend(rotulo.cpu().numpy())
            y_pred.extend(ypred.cpu().numpy())

    r2 = r2_score(y_true, y_pred)
    print("Precisão R²: %.2f %% \n" % (r2*100))
    return r2

"""# **Hiperparâmetros, e leitura do dataset**

Hiperparametros são parametros definidos para treinar o modelo, sendo eles



*   **epoch_num** -> Número de vezes que o conjunto de função de treinamento irá ocorrer
*   **lr** -> Taxa de aprendizado é a taxa da qual o modelo irá ajustar os parametros no treinamento

*   **weight_decay** -> Regularização do modelo, ou Penalidade L2, evita que os pesos do modelo sejam ajustados com valores muito altos

*   **num_workers** -> Numero de threads do DataLoader

*   **batch_size** -> Quantidade de exemplos, ou no caso linhas, que serão utilizadas em cada iteração do treinamento


Após a definição dos hiperparametros, o código verifica se a GPU está disponível para uso, será importante usar a GPU para o treinamento do modelo.
E por fim, o código solicita o upload do DataSet e mostra uma pequena amostra do arquivo que foi feito o upload.
"""

# Hiperparâmetros.
args = {
    'epoch_num': 1500,    # Número de épocas.
    'lr': 2e-5,           # Taxa de aprendizado.
    'weight_decay': 1e-3, # Penalidade L2 (Regularização).
    'num_workers': 5,     # Número de threads do dataloader.
    'batch_size': 20,     # Tamanho do batch.
}

if torch.cuda.is_available():
    args['device'] = torch.device('cuda')
else:
    args['device'] = torch.device('cpu')

print("Device -> " + str(args['device']))

uploaded = files.upload()

df = pd.read_csv('ds_ingredientes_V4.csv', sep=';')
df.head()

"""# **Instanciação da rede e criação dos DataLoaders**

Primeiramente, pra criação dos DataLoaders, é feito um embaralhamento dos dados, para que a rede seja treinada da forma mais adequada possível. Depois do embaralhamento dos dados, esses dados embaralhados são transformados em um CSV que será lido pela classe Ingrediente.

Assim, é definido o tamanho da entrada, o tamanho da camada escondida, e o tamanho da saída, e é criada a rede.

Após isso, é definido o **critério**, que no caso é o **L1Loss**, e o **otimizador**, que no caso é o **Adam**.


"""

torch.manual_seed(1)
indices = torch.randperm(len(df)).tolist()

train_size = int(0.8*len(df))
df_train = df.iloc[indices[:train_size]]
df_test  = df.iloc[indices[train_size:]]

df_train.to_csv('ing_train.csv',index=False)
df_test.to_csv('ing_test.csv',index=False)

dataset = Ingrediente('ing_train.csv')

train_set = Ingrediente('ing_train.csv')
test_set  = Ingrediente('ing_test.csv')

train_loader = DataLoader(train_set,
                          args['batch_size'],
                          num_workers=args['num_workers'],
                          shuffle=True)
test_loader = DataLoader(test_set,
                         args['batch_size'],
                         num_workers=args['num_workers'],
                         shuffle=False)

input_size  = train_set[0][0].size(0)
hidden_size = 256
out_size    = 1

net = MLP(input_size, hidden_size, out_size).to(args['device'])
print(net)

criterion = nn.L1Loss().to(args['device'])

optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])

"""# Fase de treinamentos

Nesta parte do código se inicia os treinamentos, onde há um laço que irá iterar sobre o numero de épocas, chamando as funções de treino, validacao e calculo de precisão.
Os retornos são armazenados para composição de gráficos
"""

train_losses_mean, test_losses_mean = [], []
train_losses, test_losses = [], []
accuracy = []
for epoch in range(args['epoch_num']):

    # Train
    train_loss = train(train_loader, net, epoch)
    train_losses.append(train_loss)
    train_losses_mean.append((epoch, train_loss))

    # Validate
    test_loss = validate(test_loader, net, epoch)
    test_losses.append(test_loss)
    test_losses_mean.append((epoch, test_loss))

    #Acuracia
    accuracy.append(calculate_precision(test_loader, net))

"""# Gráfico de convergência

Neste trecho de código é utilizado a classe **matplotlib** para fazer um gráfico de convergencia, que compara a loss ao longo das épocas.
"""

pickle.dump(net, open('Modelo_Treinado.sav', 'wb'))

# Gera gráfico de convergencia
plt.figure(figsize=(20, 6))

plt.plot(*zip(*train_losses_mean), label='Training Loss', linestyle='-', linewidth=2)

plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Gráfico de convergência')
plt.grid()
plt.show()

"""# Gráfico Precisão R² x Epoch

Aqui temos um gráfico parecido com o anterior, porém, é mostrado a precisão R² ao longo das épocas.
"""

epochs = list(range(args['epoch_num']))


plt.figure(figsize=(20, 8))
plt.plot(epochs, accuracy, label='Precisão R²', linestyle='-', markersize=3, color='orange')
plt.title('Precisão R² x Epoch')
plt.xlabel('Época')
plt.ylabel('Precisão R²')
plt.legend()
plt.grid(True)

plt.show()

"""# Testes e predições

Neste trecho do código, o programa irá solicitar o upload de um arquivo, neste caso foi feito o upload de um arquivo com 59 linhas para teste, com todos os ingredientes. Após o upload, os dados são embaralhados e é feito um gráfico de disperção separado por ingrediente e uma tabela com o valor real x valor predito.
"""

net = pickle.load(open('Modelo_Treinado.sav', 'rb'))

uploaded = files.upload()


dfTeste = pd.read_csv('DS_TESTE.csv', sep=';')

# Embaralha DF
torch.manual_seed(2)
indices = torch.randperm(len(dfTeste)).tolist()
test_size = int(len(dfTeste))
dfTeste = dfTeste.iloc[indices[:test_size]]

dfTeste.to_csv('DS_TESTE_IMP.csv',index=False)

teste = Ingrediente('DS_TESTE_IMP.csv');

Xtest = torch.stack([tup[0] for tup in teste])
Xtest = Xtest.to(args['device'])


ytest = torch.stack([tup[1] for tup in teste])
ypred = net(Xtest).cpu().data

data = torch.cat((ypred, ytest), axis=1)


# -------------------------- Gera Gráfico de disperção -----------------------------
df_results = pd.DataFrame(data, columns=['Predito', 'Real'])


Xtest_df = pd.DataFrame(Xtest.cpu().numpy(), columns=["ID Ingrediente","Estação do Ano","Clima","Dia da Semana","Feriado","Dia do Mês","Valor do ingrediente", "Consumo em Receitas"])


df_results_with_input = pd.concat([Xtest_df, pd.DataFrame(data, columns=['Predito', 'Real'])], axis=1)

id_color_map = {
    1: 'r',
    2: 'g',
    3: 'b',
    4: 'c',
    5: 'm',
    6: 'y',
    7: 'k',
    8: 'grey'
}

id_name_map = {
    1: 'Carnes',
    2: 'Molho',
    3: 'Farinha',
    4: 'Massas',
    5: 'Ovo',
    6: 'Queijo',
    7: 'Legumes',
    8: 'Salada'
}


for id_val, color in id_color_map.items():
    subset = df_results_with_input[df_results_with_input['ID Ingrediente'] == id_val]
    plt.scatter(subset['Predito'], subset['Real'], c=color, label=id_name_map[id_val])

plt.xlabel('Valores Reais')
plt.ylabel('Valores Previstos')
plt.legend()
plt.title('Gráfico de Dispersão por Ingrediente')

df_sample = df_results.sample(n=59)
ytest_values = df_sample['Predito'].values
ypred_values = df_sample['Real'].values
plt.plot([min(ytest_values), max(ytest_values)], [min(ytest_values), max(ytest_values)], color='orange', linestyle='--', linewidth=2, alpha=0.7)
plt.show()

# --------------------- Tabela ----------------------------

df_results = pd.DataFrame(data, columns=['Predito', 'Real'])

df_results_with_input = pd.concat([Xtest_df, df_results], axis=1)

pd.set_option('display.max_rows', None)

df_results_with_input.head(59)

"""# Teste com mais valores

Por fim, é feito um teste com 20% do tamanho do dataset que foi escolhido para treino, e é gerado um gráfico de disperção com esse teste.
"""

Xtest = torch.stack([tup[0] for tup in test_set])
Xtest = Xtest.to(args['device'])


ytest = torch.stack([tup[1] for tup in test_set])
ypred = net(Xtest).cpu().data

data = torch.cat((ypred, ytest), axis=1)

df_results = pd.DataFrame(data, columns=['ypred', 'ytest'])



df_sample = df_results.sample(n=len(df_results))
ytest_values = df_sample['ytest'].values
ypred_values = df_sample['ypred'].values

# --------------------- Gráfico de disperção 2 ----------------------------
plt.figure(figsize=(10, 5))
plt.scatter(ytest_values, ypred_values, alpha=0.5)
plt.title('Gráfico de disperção - Testado')
plt.xlabel('Valores Reais')
plt.ylabel('Valores Previstos')
plt.grid(True)

plt.plot([min(ytest_values), max(ytest_values)], [min(ytest_values), max(ytest_values)], color='red', linestyle='--', linewidth=2)

plt.show()